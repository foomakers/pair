# Performance Gates Implementation

## üéØ **PURPOSE**

Systematic implementation of performance gates that enforce quality standards at critical stages of the development lifecycle, preventing performance regressions and ensuring consistent user experience through automated validation and enforcement mechanisms.

## üîß **PERFORMANCE GATES FRAMEWORK**

### **Gate Definition and Thresholds**

**Core Web Vitals Gates**

```javascript
const PERFORMANCE_GATES = {
  // Development Gates (stricter for early detection)
  development: {
    fcp: { target: 1500, critical: 2000 }, // First Contentful Paint
    lcp: { target: 2000, critical: 2500 }, // Largest Contentful Paint
    cls: { target: 0.05, critical: 0.1 }, // Cumulative Layout Shift
    fid: { target: 80, critical: 100 }, // First Input Delay
    tbt: { target: 200, critical: 300 }, // Total Blocking Time
    si: { target: 2500, critical: 3000 }, // Speed Index
  },

  // Staging Gates (production-like validation)
  staging: {
    fcp: { target: 1800, critical: 2500 },
    lcp: { target: 2500, critical: 3000 },
    cls: { target: 0.1, critical: 0.25 },
    fid: { target: 100, critical: 300 },
    tbt: { target: 300, critical: 600 },
    si: { target: 3000, critical: 4000 },
  },

  // Production Gates (monitoring and alerting)
  production: {
    fcp: { target: 1800, critical: 3000 },
    lcp: { target: 2500, critical: 4000 },
    cls: { target: 0.1, critical: 0.25 },
    fid: { target: 100, critical: 300 },
    tbt: { target: 300, critical: 600 },
    si: { target: 3000, critical: 5000 },
  },
}
```

**Resource Budget Gates**

```javascript
const RESOURCE_GATES = {
  development: {
    totalSize: { target: 1024 * 1024 * 1.5, critical: 1024 * 1024 * 2 }, // 1.5MB target, 2MB critical
    jsSize: { target: 1024 * 400, critical: 1024 * 600 }, // 400KB target, 600KB critical
    cssSize: { target: 1024 * 80, critical: 1024 * 120 }, // 80KB target, 120KB critical
    imageSize: { target: 1024 * 800, critical: 1024 * 1200 }, // 800KB target, 1.2MB critical
    requests: { target: 40, critical: 60 }, // 40 requests target, 60 critical
    thirdPartySize: { target: 1024 * 200, critical: 1024 * 400 }, // 200KB target, 400KB critical
  },

  staging: {
    totalSize: { target: 1024 * 1024 * 2, critical: 1024 * 1024 * 3 },
    jsSize: { target: 1024 * 500, critical: 1024 * 800 },
    cssSize: { target: 1024 * 100, critical: 1024 * 150 },
    imageSize: { target: 1024 * 1024, critical: 1024 * 1024 * 1.5 },
    requests: { target: 50, critical: 80 },
    thirdPartySize: { target: 1024 * 300, critical: 1024 * 500 },
  },

  production: {
    totalSize: { target: 1024 * 1024 * 2.5, critical: 1024 * 1024 * 4 },
    jsSize: { target: 1024 * 600, critical: 1024 * 1024 },
    cssSize: { target: 1024 * 120, critical: 1024 * 200 },
    imageSize: { target: 1024 * 1024 * 1.2, critical: 1024 * 1024 * 2 },
    requests: { target: 60, critical: 100 },
    thirdPartySize: { target: 1024 * 400, critical: 1024 * 600 },
  },
}
```

## üìä **GATE IMPLEMENTATION**

### **Pre-commit Performance Gate**

**Git Hook Implementation**

```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "Running performance checks..."

# Check if performance-critical files changed
CRITICAL_FILES=$(git diff --cached --name-only | grep -E '\.(js|jsx|ts|tsx|css|scss|html)$')

if [ -z "$CRITICAL_FILES" ]; then
  echo "No performance-critical files changed. Skipping performance checks."
  exit 0
fi

# Run bundle size analysis
npm run build:analyze > /dev/null 2>&1
if [ $? -ne 0 ]; then
  echo "‚ùå Build failed. Performance gate blocked."
  exit 1
fi

# Check bundle size
node scripts/check-bundle-size.js
BUNDLE_CHECK=$?

# Check lighthouse score on local build
npm run perf:check-local
PERF_CHECK=$?

if [ $BUNDLE_CHECK -ne 0 ] || [ $PERF_CHECK -ne 0 ]; then
  echo "‚ùå Performance gate failed. Commit blocked."
  echo "Run 'npm run perf:fix' to analyze and fix performance issues."
  exit 1
fi

echo "‚úÖ Performance gate passed."
exit 0
```

**Bundle Size Check Script**

```javascript
// scripts/check-bundle-size.js
const fs = require('fs')
const path = require('path')
const { gzipSizeSync } = require('gzip-size')

class BundleSizeGate {
  constructor() {
    this.buildDir = './dist'
    this.reportPath = './bundle-report.json'
    this.thresholds = RESOURCE_GATES.development
  }

  async checkBundleSize() {
    try {
      const bundleAnalysis = this.analyzeBundles()
      const gateResult = this.evaluateGate(bundleAnalysis)

      this.generateReport(bundleAnalysis, gateResult)

      return gateResult.passed
    } catch (error) {
      console.error('Bundle size check failed:', error)
      return false
    }
  }

  analyzeBundles() {
    const analysis = {
      totalSize: 0,
      gzippedSize: 0,
      jsSize: 0,
      cssSize: 0,
      imageSize: 0,
      otherSize: 0,
      files: [],
    }

    const walkDir = dir => {
      const files = fs.readdirSync(dir)

      files.forEach(file => {
        const filePath = path.join(dir, file)
        const stat = fs.statSync(filePath)

        if (stat.isDirectory()) {
          walkDir(filePath)
        } else {
          const content = fs.readFileSync(filePath)
          const size = stat.size
          const gzippedSize = gzipSizeSync(content)
          const ext = path.extname(file)

          const fileAnalysis = {
            path: filePath,
            size,
            gzippedSize,
            type: this.getFileType(ext),
          }

          analysis.files.push(fileAnalysis)
          analysis.totalSize += size
          analysis.gzippedSize += gzippedSize

          switch (fileAnalysis.type) {
            case 'js':
              analysis.jsSize += size
              break
            case 'css':
              analysis.cssSize += size
              break
            case 'image':
              analysis.imageSize += size
              break
            default:
              analysis.otherSize += size
          }
        }
      })
    }

    if (fs.existsSync(this.buildDir)) {
      walkDir(this.buildDir)
    }

    return analysis
  }

  getFileType(ext) {
    const jsExts = ['.js', '.mjs', '.jsx']
    const cssExts = ['.css', '.scss', '.sass']
    const imageExts = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.avif']

    if (jsExts.includes(ext)) return 'js'
    if (cssExts.includes(ext)) return 'css'
    if (imageExts.includes(ext)) return 'image'
    return 'other'
  }

  evaluateGate(analysis) {
    const violations = []
    const warnings = []

    // Check total size
    if (analysis.totalSize > this.thresholds.totalSize.critical) {
      violations.push({
        type: 'total-size',
        current: analysis.totalSize,
        threshold: this.thresholds.totalSize.critical,
        severity: 'critical',
      })
    } else if (analysis.totalSize > this.thresholds.totalSize.target) {
      warnings.push({
        type: 'total-size',
        current: analysis.totalSize,
        threshold: this.thresholds.totalSize.target,
        severity: 'warning',
      })
    }

    // Check JavaScript size
    if (analysis.jsSize > this.thresholds.jsSize.critical) {
      violations.push({
        type: 'js-size',
        current: analysis.jsSize,
        threshold: this.thresholds.jsSize.critical,
        severity: 'critical',
      })
    } else if (analysis.jsSize > this.thresholds.jsSize.target) {
      warnings.push({
        type: 'js-size',
        current: analysis.jsSize,
        threshold: this.thresholds.jsSize.target,
        severity: 'warning',
      })
    }

    // Check CSS size
    if (analysis.cssSize > this.thresholds.cssSize.critical) {
      violations.push({
        type: 'css-size',
        current: analysis.cssSize,
        threshold: this.thresholds.cssSize.critical,
        severity: 'critical',
      })
    }

    // Check image size
    if (analysis.imageSize > this.thresholds.imageSize.critical) {
      violations.push({
        type: 'image-size',
        current: analysis.imageSize,
        threshold: this.thresholds.imageSize.critical,
        severity: 'critical',
      })
    }

    return {
      passed: violations.length === 0,
      violations,
      warnings,
      score: this.calculateScore(violations, warnings),
    }
  }

  calculateScore(violations, warnings) {
    const baseScore = 100
    const violationPenalty = violations.length * 25
    const warningPenalty = warnings.length * 10

    return Math.max(0, baseScore - violationPenalty - warningPenalty)
  }

  generateReport(analysis, gateResult) {
    console.log('Bundle Size Gate Report')
    console.log('======================')
    console.log(`Status: ${gateResult.passed ? '‚úÖ PASSED' : '‚ùå FAILED'}`)
    console.log(`Score: ${gateResult.score}/100`)
    console.log(
      `Total Size: ${this.formatBytes(analysis.totalSize)} (gzipped: ${this.formatBytes(
        analysis.gzippedSize,
      )})`,
    )
    console.log(`JavaScript: ${this.formatBytes(analysis.jsSize)}`)
    console.log(`CSS: ${this.formatBytes(analysis.cssSize)}`)
    console.log(`Images: ${this.formatBytes(analysis.imageSize)}`)
    console.log(`Other: ${this.formatBytes(analysis.otherSize)}`)

    if (gateResult.violations.length > 0) {
      console.log('\n‚ùå Critical Violations:')
      gateResult.violations.forEach(violation => {
        console.log(
          `  ${violation.type}: ${this.formatBytes(violation.current)} > ${this.formatBytes(
            violation.threshold,
          )}`,
        )
      })
    }

    if (gateResult.warnings.length > 0) {
      console.log('\n‚ö†Ô∏è  Warnings:')
      gateResult.warnings.forEach(warning => {
        console.log(
          `  ${warning.type}: ${this.formatBytes(warning.current)} > ${this.formatBytes(
            warning.threshold,
          )}`,
        )
      })
    }

    // Show largest files
    const largestFiles = analysis.files.sort((a, b) => b.size - a.size).slice(0, 5)

    if (largestFiles.length > 0) {
      console.log('\nüì¶ Largest Files:')
      largestFiles.forEach(file => {
        console.log(
          `  ${file.path}: ${this.formatBytes(file.size)} (${this.formatBytes(
            file.gzippedSize,
          )} gzipped)`,
        )
      })
    }

    // Save detailed report
    const report = {
      timestamp: new Date().toISOString(),
      analysis,
      gateResult,
    }

    fs.writeFileSync(this.reportPath, JSON.stringify(report, null, 2))
  }

  formatBytes(bytes) {
    if (bytes === 0) return '0 B'
    const k = 1024
    const sizes = ['B', 'KB', 'MB', 'GB']
    const i = Math.floor(Math.log(bytes) / Math.log(k))
    return parseFloat((bytes / Math.pow(k, i)).toFixed(1)) + ' ' + sizes[i]
  }
}

// Run bundle size check
if (require.main === module) {
  const gate = new BundleSizeGate()
  gate.checkBundleSize().then(passed => {
    process.exit(passed ? 0 : 1)
  })
}

module.exports = BundleSizeGate
```

### **CI/CD Performance Gates**

**GitHub Actions Performance Gate**

```yaml
name: Performance Gate

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main]

jobs:
  performance-gate:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Bundle Size Gate
        run: |
          npm run bundle:analyze
          node scripts/check-bundle-size.js

      - name: Start test server
        run: |
          npm start &
          npx wait-on http://localhost:3000 --timeout 60000

      - name: Lighthouse Performance Gate
        id: lighthouse
        uses: treosh/lighthouse-ci-action@v9
        with:
          configPath: './lighthouserc.json'
          uploadArtifacts: true
          temporaryPublicStorage: true
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}

      - name: WebPageTest Gate
        run: |
          npm install -g webpagetest
          node scripts/wpt-performance-gate.js
        env:
          WPT_API_KEY: ${{ secrets.WPT_API_KEY }}

      - name: Performance Regression Check
        run: node scripts/performance-regression-check.js

      - name: Update Performance Database
        if: success()
        run: node scripts/update-performance-db.js
        env:
          DATABASE_URL: ${{ secrets.PERFORMANCE_DB_URL }}

      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const performanceData = JSON.parse(fs.readFileSync('./performance-results.json'));

            const comment = `## üöÄ Performance Gate Results

            **Status**: ${performanceData.passed ? '‚úÖ PASSED' : '‚ùå FAILED'}
            **Score**: ${performanceData.score}/100

            ### Core Web Vitals
            - **FCP**: ${performanceData.metrics.fcp}ms (target: <1800ms)
            - **LCP**: ${performanceData.metrics.lcp}ms (target: <2500ms)
            - **CLS**: ${performanceData.metrics.cls} (target: <0.1)
            - **TBT**: ${performanceData.metrics.tbt}ms (target: <300ms)

            ### Bundle Analysis
            - **Total Size**: ${performanceData.bundleSize.total} (${performanceData.bundleSize.gzipped} gzipped)
            - **JavaScript**: ${performanceData.bundleSize.js}
            - **CSS**: ${performanceData.bundleSize.css}

            ${performanceData.violations.length > 0 ? '### ‚ùå Violations\n' + performanceData.violations.map(v => `- ${v.type}: ${v.message}`).join('\n') : ''}

            ${performanceData.recommendations.length > 0 ? '### üí° Recommendations\n' + performanceData.recommendations.map(r => `- ${r}`).join('\n') : ''}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Fail on Performance Gate Violation
        if: steps.lighthouse.outcome == 'failure'
        run: |
          echo "Performance gate failed. Check the results above."
          exit 1
```

**Performance Regression Detection**

```javascript
// scripts/performance-regression-check.js
class PerformanceRegressionDetector {
  constructor() {
    this.regressionThresholds = {
      fcp: 0.2, // 20% regression threshold
      lcp: 0.2,
      cls: 0.5, // 50% regression for CLS (more sensitive)
      tbt: 0.3, // 30% regression for TBT
      si: 0.25, // 25% regression for Speed Index
    }
    this.baselineSource = process.env.CI_DEFAULT_BRANCH || 'main'
  }

  async checkRegression() {
    try {
      const currentMetrics = await this.getCurrentMetrics()
      const baselineMetrics = await this.getBaselineMetrics()

      if (!baselineMetrics) {
        console.log('No baseline metrics found. Skipping regression check.')
        return { passed: true, reason: 'no-baseline' }
      }

      const regressionAnalysis = this.analyzeRegression(currentMetrics, baselineMetrics)
      this.reportRegression(regressionAnalysis)

      return regressionAnalysis
    } catch (error) {
      console.error('Regression check failed:', error)
      return { passed: false, error: error.message }
    }
  }

  async getCurrentMetrics() {
    // Read current performance metrics from Lighthouse results
    const lighthouseResults = JSON.parse(
      require('fs').readFileSync('./lighthouse-results.json', 'utf8'),
    )

    return this.extractMetrics(lighthouseResults)
  }

  async getBaselineMetrics() {
    // Fetch baseline metrics from performance database or previous runs
    const response = await fetch(`${process.env.PERFORMANCE_API}/baseline/${this.baselineSource}`)

    if (!response.ok) {
      return null
    }

    const baseline = await response.json()
    return baseline.metrics
  }

  extractMetrics(lighthouseResults) {
    const audits = lighthouseResults.audits

    return {
      fcp: audits['first-contentful-paint'].numericValue,
      lcp: audits['largest-contentful-paint'].numericValue,
      cls: audits['cumulative-layout-shift'].numericValue,
      tbt: audits['total-blocking-time'].numericValue,
      si: audits['speed-index'].numericValue,
    }
  }

  analyzeRegression(current, baseline) {
    const regressions = []
    const improvements = []

    Object.keys(this.regressionThresholds).forEach(metric => {
      const currentValue = current[metric]
      const baselineValue = baseline[metric]
      const threshold = this.regressionThresholds[metric]

      const change = (currentValue - baselineValue) / baselineValue
      const regression = change > threshold
      const improvement = change < -threshold

      if (regression) {
        regressions.push({
          metric,
          current: currentValue,
          baseline: baselineValue,
          change: change * 100,
          threshold: threshold * 100,
        })
      } else if (improvement) {
        improvements.push({
          metric,
          current: currentValue,
          baseline: baselineValue,
          change: change * 100,
        })
      }
    })

    return {
      passed: regressions.length === 0,
      regressions,
      improvements,
      summary: this.generateSummary(regressions, improvements),
    }
  }

  generateSummary(regressions, improvements) {
    if (regressions.length === 0 && improvements.length === 0) {
      return 'No significant performance changes detected.'
    }

    let summary = ''

    if (regressions.length > 0) {
      summary += `‚ö†Ô∏è ${regressions.length} performance regression(s) detected:\n`
      regressions.forEach(reg => {
        summary += `  - ${reg.metric.toUpperCase()}: ${reg.change.toFixed(1)}% slower\n`
      })
    }

    if (improvements.length > 0) {
      summary += `üéâ ${improvements.length} performance improvement(s) detected:\n`
      improvements.forEach(imp => {
        summary += `  - ${imp.metric.toUpperCase()}: ${Math.abs(imp.change).toFixed(1)}% faster\n`
      })
    }

    return summary
  }

  reportRegression(analysis) {
    console.log('Performance Regression Analysis')
    console.log('==============================')
    console.log(`Status: ${analysis.passed ? '‚úÖ NO REGRESSIONS' : '‚ùå REGRESSIONS DETECTED'}`)
    console.log(analysis.summary)

    if (analysis.regressions.length > 0) {
      console.log('\nDetailed Regression Analysis:')
      analysis.regressions.forEach(reg => {
        console.log(`${reg.metric.toUpperCase()}:`)
        console.log(`  Current: ${reg.current.toFixed(2)}ms`)
        console.log(`  Baseline: ${reg.baseline.toFixed(2)}ms`)
        console.log(`  Change: ${reg.change.toFixed(1)}% (threshold: ${reg.threshold}%)`)
      })
    }

    // Save regression report
    require('fs').writeFileSync(
      './performance-regression-report.json',
      JSON.stringify(analysis, null, 2),
    )
  }
}

// Run regression check
if (require.main === module) {
  const detector = new PerformanceRegressionDetector()
  detector.checkRegression().then(result => {
    process.exit(result.passed ? 0 : 1)
  })
}

module.exports = PerformanceRegressionDetector
```

## üöÄ **PRODUCTION PERFORMANCE GATES**

### **Real-time Monitoring Gates**

**Performance Alert System**

```javascript
class ProductionPerformanceGates {
  constructor() {
    this.metrics = new Map()
    this.alertThresholds = PERFORMANCE_GATES.production
    this.alertCooldown = 5 * 60 * 1000 // 5 minutes
    this.lastAlerts = new Map()
  }

  processMetric(metric) {
    const { name, value, timestamp, metadata } = metric

    // Store metric
    this.metrics.set(name, { value, timestamp, metadata })

    // Check gate thresholds
    this.checkGate(name, value, metadata)

    // Update rolling averages
    this.updateRollingAverages(name, value)
  }

  checkGate(metricName, value, metadata) {
    const threshold = this.alertThresholds[metricName]
    if (!threshold) return

    const severity = this.evaluateSeverity(value, threshold)

    if (severity && this.shouldAlert(metricName, severity)) {
      this.triggerAlert(metricName, value, threshold, severity, metadata)
    }
  }

  evaluateSeverity(value, threshold) {
    if (value > threshold.critical) {
      return 'critical'
    } else if (value > threshold.target) {
      return 'warning'
    }
    return null
  }

  shouldAlert(metricName, severity) {
    const key = `${metricName}-${severity}`
    const lastAlert = this.lastAlerts.get(key)
    const now = Date.now()

    if (!lastAlert || now - lastAlert > this.alertCooldown) {
      this.lastAlerts.set(key, now)
      return true
    }

    return false
  }

  triggerAlert(metricName, value, threshold, severity, metadata) {
    const alert = {
      timestamp: new Date().toISOString(),
      metric: metricName,
      value,
      threshold: threshold[severity],
      severity,
      metadata,
      message: this.generateAlertMessage(metricName, value, threshold, severity),
    }

    console.warn('Performance Alert:', alert)

    // Send to monitoring systems
    this.sendToMonitoring(alert)
    this.sendToSlack(alert)
    this.updateDashboard(alert)
  }

  generateAlertMessage(metricName, value, threshold, severity) {
    const emoji = severity === 'critical' ? 'üö®' : '‚ö†Ô∏è'
    const formatted = this.formatMetricValue(metricName, value)
    const thresholdFormatted = this.formatMetricValue(metricName, threshold[severity])

    return `${emoji} ${metricName.toUpperCase()} performance ${severity}: ${formatted} (threshold: ${thresholdFormatted})`
  }

  formatMetricValue(metricName, value) {
    switch (metricName) {
      case 'cls':
        return value.toFixed(3)
      case 'fcp':
      case 'lcp':
      case 'fid':
      case 'tbt':
      case 'si':
        return `${value.toFixed(0)}ms`
      default:
        return value.toString()
    }
  }

  updateRollingAverages(metricName, value) {
    // Implement rolling average calculation for trend analysis
    const key = `${metricName}-rolling`
    let rolling = this.metrics.get(key) || { values: [], average: 0 }

    rolling.values.push(value)

    // Keep only last 100 values
    if (rolling.values.length > 100) {
      rolling.values = rolling.values.slice(-100)
    }

    rolling.average = rolling.values.reduce((sum, val) => sum + val, 0) / rolling.values.length

    this.metrics.set(key, rolling)
  }

  sendToMonitoring(alert) {
    // Send to monitoring service (DataDog, New Relic, etc.)
    if (process.env.MONITORING_WEBHOOK) {
      fetch(process.env.MONITORING_WEBHOOK, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          alert_type: 'performance',
          ...alert,
        }),
      }).catch(console.error)
    }
  }

  sendToSlack(alert) {
    // Send to Slack
    if (process.env.SLACK_WEBHOOK) {
      const color = alert.severity === 'critical' ? 'danger' : 'warning'

      fetch(process.env.SLACK_WEBHOOK, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          attachments: [
            {
              color,
              title: 'Performance Alert',
              text: alert.message,
              fields: [
                { title: 'Metric', value: alert.metric, short: true },
                {
                  title: 'Value',
                  value: this.formatMetricValue(alert.metric, alert.value),
                  short: true,
                },
                {
                  title: 'Threshold',
                  value: this.formatMetricValue(alert.metric, alert.threshold),
                  short: true,
                },
                { title: 'Severity', value: alert.severity, short: true },
              ],
              timestamp: Math.floor(Date.now() / 1000),
            },
          ],
        }),
      }).catch(console.error)
    }
  }

  updateDashboard(alert) {
    // Update real-time dashboard
    if (process.env.DASHBOARD_WEBSOCKET) {
      // Send real-time update to dashboard
      this.sendWebSocketUpdate({
        type: 'performance-alert',
        alert,
      })
    }
  }
}

// Initialize production gates
const productionGates = new ProductionPerformanceGates()

// Export for use in monitoring system
module.exports = productionGates
```

---

_Performance gates provide systematic quality enforcement throughout the development lifecycle, ensuring consistent user experience and preventing performance regressions through automated validation and monitoring._
